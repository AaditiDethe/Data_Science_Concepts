# Confusion Matrix

A Confusion Matrix is a performance measurement tool used to evaluate the accuracy of a classification algorithm. It provides a summary of the prediction results by comparing the actual labels with the predicted labels.

Key Metrics Derived from the Confusion Matrix

Accuracy: Measures overall correctness of the model.


Precision: Measures the proportion of correct positive predictions.


Recall (Sensitivity): Measures the proportion of actual positives correctly identified.


F1-Score: The harmonic mean of precision and recall.


Specificity: Measures the proportion of actual negatives correctly identified.
